import requests
from bs4 import BeautifulSoup
import pandas as pd
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
import datetime
import os

# âœ… NLTK ê°ì„± ë¶„ì„ê¸° ë‹¤ìš´ë¡œë“œ ë° ì´ˆê¸°í™”
nltk.download("vader_lexicon")
sia = SentimentIntensityAnalyzer()

# ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜
def get_naver_news():
    url = "https://news.naver.com/main/ranking/popularDay.naver"
    headers = {"User-Agent": "Mozilla/5.0"}

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: ìƒíƒœ ì½”ë“œ {response.status_code}")
        return []

    soup = BeautifulSoup(response.text, "html.parser")
    articles = []

    for item in soup.select("div.list_content a"):
        title = item.get_text().strip()
        link = "https://" + item["href"][7:]
        sentiment_score = sia.polarity_scores(title)["compound"]
        sentiment = "ê¸ì •" if sentiment_score > 0 else "ë¶€ì •" if sentiment_score < 0 else "ì¤‘ë¦½"

        articles.append({"title": title, "link": link, "sentiment": sentiment})

    if len(articles) == 0:
        print("âŒ í¬ë¡¤ë§ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´íŠ¸ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
    else:
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(articles)}ê°œì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ë¨.")

    return articles

# ë°ì´í„° ì €ì¥
def save_to_csv(data):
    if len(data) == 0:
        print("ğŸš¨ ì €ì¥í•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. CSV íŒŒì¼ì„ ë§Œë“¤ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None

    df = pd.DataFrame(data)
    today = datetime.date.today().strftime("%Y-%m-%d")
    filename = f"news_{today}.csv"
    df.to_csv(filename, index=False, encoding="utf-8")
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {filename}")
    return filename

# âœ… README.md ì—…ë°ì´íŠ¸ í•¨ìˆ˜
def update_readme(news_data):
    if len(news_data) == 0:
        print("ğŸš¨ README.mdë¥¼ ì—…ë°ì´íŠ¸í•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    today = datetime.date.today().strftime("%Y-%m-%d")

    # âœ… ê°ì„± ë¶„ì„ ê²°ê³¼ ìš”ì•½
    sentiment_counts = {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0}
    for news in news_data:
        sentiment_counts[news["sentiment"]] += 1

    sentiment_summary = (
        f"ğŸŸ¢ ê¸ì • ë‰´ìŠ¤: {sentiment_counts['ê¸ì •']}ê°œ | ğŸ”´ ë¶€ì • ë‰´ìŠ¤: {sentiment_counts['ë¶€ì •']}ê°œ | âšª ì¤‘ë¦½ ë‰´ìŠ¤: {sentiment_counts['ì¤‘ë¦½']}ê°œ"
    )

    # âœ… ìµœì‹  ë‰´ìŠ¤ 5ê°œë§Œ ì„ íƒ
    latest_news = news_data[:5]
    
    # âœ… Markdown í…Œì´ë¸” ìƒì„± (íŒŒì´í”„ ë¬¸ì `|`ë¥¼ ì•ˆì „í•˜ê²Œ ë³€í™˜)
    # news_table = "| No | Headline | Sentiment |\n|----|---------|----------|\n"
    # for i, news in enumerate(latest_news, 1):
    #     safe_title = news['title'].replace("|", "ï½œ")  # ğŸ› ï¸ `|`ë¥¼ `ï½œ`(ì „ê° ë¬¸ì)ë¡œ ë³€í™˜í•˜ì—¬ Markdown ì¶©ëŒ ë°©ì§€
    #     sentiment_icon = "ğŸ˜Š" if news["sentiment"] == "ê¸ì •" else "ğŸ˜¡" if news["sentiment"] == "ë¶€ì •" else "ğŸ˜"
    #     news_table += f"| {i} | [{safe_title}]({news['link']}) | {sentiment_icon} {news['sentiment']} |\n"

    # âœ… Markdown í…Œì´ë¸” ìƒì„± (ì¤„ë°”ê¿ˆ ë° ê³µë°± ì²˜ë¦¬)
    news_table = """<table>
    <tr>
        <th>No</th>
        <th>Headline</th>
        <th>Sentiment</th>
    </tr>"""
    
    for i, news in enumerate(latest_news, 1):
        safe_title = news['title'].replace("|", "ï½œ")  # ğŸ› ï¸ `|`ë¥¼ `ï½œ`ë¡œ ë³€í™˜í•˜ì—¬ Markdown ì¶©ëŒ ë°©ì§€
        sentiment_icon = "ğŸ˜Š" if news["sentiment"] == "ê¸ì •" else "ğŸ˜¡" if news["sentiment"] == "ë¶€ì •" else "ğŸ˜"
        news_table += f"""
    <tr>
        <td>{i}</td>
        <td><a href="{news['link']}">{safe_title}</a></td>
        <td>{sentiment_icon} {news['sentiment']}</td>
    </tr>"""

    news_table += "</table>"

    # âœ… README.md ì—…ë°ì´íŠ¸ (ìµœì‹  ë‰´ìŠ¤ë§Œ ìœ ì§€)
    readme_content = f"""# ğŸ“° News Trend Analysis

ğŸš€ This project automatically scrapes the latest news daily and updates this repository.

## ğŸ“… Latest News ({today})

ğŸ“Š **{sentiment_summary}**  

{news_table}  

ğŸ“œ **[View Full News Archive](news_archive.md)** ğŸ‘ˆ (Click here for past news)
"""

    with open("README.md", "w", encoding="utf-8") as f:
        f.write(readme_content)

    print("âœ… README.md updated successfully!")

    # âœ… ê³¼ê±° ë‰´ìŠ¤ ê¸°ë¡ì„ `news_archive.md`ì— ì €ì¥
    archive_file = "news_archive.md"
    archive_entry = f"## ğŸ“… {today}\n\n{news_table}\n---\n\n"

    if os.path.exists(archive_file):
        with open(archive_file, "r", encoding="utf-8") as f:
            old_archive = f.read()
    else:
        old_archive = "# ğŸ“œ News Archive\n\n"

    with open(archive_file, "w", encoding="utf-8") as f:
        f.write(old_archive + archive_entry)

    print("âœ… news_archive.md updated successfully!")

if __name__ == "__main__":
    news = get_naver_news()
    save_to_csv(news)
    update_readme(news)