import requests
from bs4 import BeautifulSoup
import pandas as pd
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
import datetime
import os

# âœ… NLTK ê°ì„± ë¶„ì„ê¸° ë‹¤ìš´ë¡œë“œ ë° ì´ˆê¸°í™”
nltk.download("vader_lexicon")
sia = SentimentIntensityAnalyzer()

# ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜
def get_naver_news():
    url = "https://news.naver.com/main/ranking/popularDay.naver"
    headers = {"User-Agent": "Mozilla/5.0"}

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: ìƒíƒœ ì½”ë“œ {response.status_code}")
        return []

    soup = BeautifulSoup(response.text, "html.parser")
    articles = []

    for item in soup.select("div.list_content a"):
        title = item.get_text().strip()
        link = "https://" + item["href"][7:]
        sentiment_score = sia.polarity_scores(title)["compound"]
        sentiment = "ê¸ì •" if sentiment_score > 0 else "ë¶€ì •" if sentiment_score < 0 else "ì¤‘ë¦½"

        articles.append({"title": title, "link": link, "sentiment": sentiment})

    if len(articles) == 0:
        print("âŒ í¬ë¡¤ë§ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´íŠ¸ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ìžˆìŠµë‹ˆë‹¤.")
    else:
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(articles)}ê°œì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ë¨.")

    return articles

# ë°ì´í„° ì €ìž¥
def save_to_csv(data):
    if len(data) == 0:
        print("ðŸš¨ ì €ìž¥í•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. CSV íŒŒì¼ì„ ë§Œë“¤ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None

    df = pd.DataFrame(data)
    today = datetime.date.today().strftime("%Y-%m-%d")
    filename = f"news_{today}.csv"
    df.to_csv(filename, index=False, encoding="utf-8")
    print(f"âœ… ì €ìž¥ ì™„ë£Œ: {filename}")
    return filename

# âœ… README.md ì—…ë°ì´íŠ¸ í•¨ìˆ˜
def update_readme(news_data):
    if len(news_data) == 0:
        print("ðŸš¨ README.mdë¥¼ ì—…ë°ì´íŠ¸í•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    today = datetime.date.today().strftime("%Y-%m-%d")

    # âœ… ê°ì„± ë¶„ì„ ê²°ê³¼ ìš”ì•½
    sentiment_counts = {"ê¸ì •": 0, "ë¶€ì •": 0, "ì¤‘ë¦½": 0}
    for news in news_data:
        sentiment_counts[news["sentiment"]] += 1

    sentiment_summary = (
        f"ðŸŸ¢ ê¸ì • ë‰´ìŠ¤: {sentiment_counts['ê¸ì •']}ê°œ | ðŸ”´ ë¶€ì • ë‰´ìŠ¤: {sentiment_counts['ë¶€ì •']}ê°œ | âšª ì¤‘ë¦½ ë‰´ìŠ¤: {sentiment_counts['ì¤‘ë¦½']}ê°œ"
    )

    # âœ… ìµœì‹  ë‰´ìŠ¤ 5ê°œë§Œ ì„ íƒ
    latest_news = news_data[:5]

    # âœ… Markdown í…Œì´ë¸” ìƒì„±
    # news_table = "| No | Headline | Sentiment |\n|----|---------|----------|\n"
    # for i, news in enumerate(latest_news, 1):
    #     sentiment_icon = "ðŸ˜Š" if news["sentiment"] == "ê¸ì •" else "ðŸ˜¡" if news["sentiment"] == "ë¶€ì •" else "ðŸ˜"
    #     news_table += f"| {i} | [{news['title']}]({news['link']}) | {sentiment_icon} {news['sentiment']} |\n"
    
    # âœ… Markdown í…Œì´ë¸” ìƒì„± (íŒŒì´í”„ ë¬¸ìž `|`ë¥¼ ì•ˆì „í•˜ê²Œ ë³€í™˜)
    news_table = "| No | Headline | Sentiment |\n|----|---------|----------|\n"
    for i, news in enumerate(latest_news, 1):
        safe_title = news['title'].replace("|", "ï½œ")  # ðŸ› ï¸ `|`ë¥¼ `ï½œ`(ì „ê° ë¬¸ìž)ë¡œ ë³€í™˜í•˜ì—¬ Markdown ì¶©ëŒ ë°©ì§€
        sentiment_icon = "ðŸ˜Š" if news["sentiment"] == "ê¸ì •" else "ðŸ˜¡" if news["sentiment"] == "ë¶€ì •" else "ðŸ˜"
        news_table += f"| {i} | [{safe_title}]({news['link']}) | {sentiment_icon} {news['sentiment']} |\n"

    # âœ… README.md ì—…ë°ì´íŠ¸ (ìµœì‹  ë‰´ìŠ¤ë§Œ ìœ ì§€)
    readme_content = f"""# ðŸ“° News Trend Analysis

    ðŸš€ This project automatically scrapes the latest news daily and updates this repository.

    ## ðŸ“… Latest News ({today})

    **{sentiment_summary}**

    {news_table}

    ðŸ“œ **[View Full News Archive](news_archive.md)** ðŸ‘ˆ (Click here for past news)
    """

    with open("README.md", "w", encoding="utf-8") as f:
        f.write(readme_content)

    print("âœ… README.md updated successfully!")

    # âœ… ê³¼ê±° ë‰´ìŠ¤ ê¸°ë¡ì„ `news_archive.md`ì— ì €ìž¥
    archive_file = "news_archive.md"
    archive_entry = f"## ðŸ“… {today}\n\n{news_table}\n---\n"

    if os.path.exists(archive_file):
        with open(archive_file, "r", encoding="utf-8") as f:
            old_archive = f.read()
    else:
        old_archive = "# ðŸ“œ News Archive\n\n"

    with open(archive_file, "w", encoding="utf-8") as f:
        f.write(old_archive + archive_entry)

    print("âœ… news_archive.md updated successfully!")

if __name__ == "__main__":
    news = get_naver_news()
    save_to_csv(news)
    update_readme(news)